{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SdiYblJx2ao"
   },
   "source": [
    "# Generate Candidates #\n",
    "We take candidates from a range of sources:\n",
    "* Items already interacted with in the session\n",
    "* Our covisitation matrices\n",
    "* A word2vec model\n",
    "* An ALS recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rgw3DUBDx7hN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to module: /home/jupyter/kaggle-otto-recommender-2022\n",
      "data path: /home/jupyter/kaggle-otto-recommender-2022/data\n",
      "/home/jupyter/kaggle-otto-recommender-2022/data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import data_path, path_to_module\n",
    "print(f\"path to module: {path_to_module}\")\n",
    "print(f\"data path: {data_path}\")\n",
    "import sys   \n",
    "sys.path.append(path_to_module)\n",
    "%cd {data_path}\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M9QNAWgIugYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: implicit in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from implicit) (4.64.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from implicit) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/conda/lib/python3.7/site-packages (from implicit) (1.7.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: Annoy in /opt/conda/lib/python3.7/site-packages (1.17.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting fastparquet\n",
      "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from fastparquet) (2023.1.0)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from fastparquet) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.7/site-packages (from fastparquet) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.0->fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.16.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.6.2 fastparquet-0.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install implicit\n",
    "!pip install Annoy\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bbpCdPcQOMqe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/implicit/gpu/__init__.py:14: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: CUDA driver version is insufficient for CUDA runtime version (/home/conda/feedstock_root/build_artifacts/implicit_1643471602441/work/./implicit/gpu/utils.h:71)'\n",
      "  f\"CUDA extension is built, but disabling GPU support because of '{e}'\",\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import seaborn as sns\n",
    "from otto_utils import get_train, get_test, convert_columns, save_parquet, make_directory, create_sub\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sps\n",
    "import implicit\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QZ6WwCocB3CD"
   },
   "outputs": [],
   "source": [
    "sample_prop = None\n",
    "validation = True\n",
    "covisitation = True\n",
    "cart_order = True\n",
    "also_buy = True\n",
    "als = True\n",
    "word2vec = True\n",
    "\n",
    "\n",
    "# debug = True\n",
    "\n",
    "path_to_candidate_features = './train_candidate_features' if validation else './test_candidate_features'\n",
    "n=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain_candidate_features\u001b[0m/  \u001b[01;34mvalidation\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f4EnH3CgZf89"
   },
   "outputs": [],
   "source": [
    "reduced_df = get_test(validation, sample_prop)\n",
    "reduced_df.rename(columns = {'type' : 'source'}, inplace=True)\n",
    "reduced_df['joiner'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7683577, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5soINLMjD-cK"
   },
   "source": [
    "## Calculate the candidates from items already in the basket ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train_candidate_features'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_candidate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tmAKeMmvTK5K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:01<00:00, 66.26it/s]\n"
     ]
    }
   ],
   "source": [
    "training_skeleton = reduced_df\n",
    "# Changing this from 0.1 to 0.5 impacts recall by 0.01\n",
    "training_skeleton['time_weight'] = training_skeleton.groupby('session')['aid'].transform(lambda x: np.logspace(0.1, 1, x.shape[0], base=2, endpoint=True)) - 1 \n",
    "training_skeleton['type_weight'] = 1\n",
    "training_skeleton.loc[training_skeleton.source == 'carts', 'type_weight'] = 3\n",
    "training_skeleton.loc[training_skeleton.source == 'orders', 'type_weight'] = 6\n",
    "training_skeleton['weight'] = training_skeleton['time_weight'] * training_skeleton['type_weight']\n",
    "training_skeleton = training_skeleton.groupby(['session', 'aid'], as_index=False).agg({'weight' : 'sum'})\n",
    "training_skeleton.sort_values(by=['session', 'weight'], ascending=[True, False], inplace=True)\n",
    "training_skeleton['n_basket'] = training_skeleton.groupby('session').cumcount() + 1\n",
    "training_skeleton = convert_columns(training_skeleton)\n",
    "\n",
    "save_parquet(training_skeleton, f'{path_to_candidate_features}/basket', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMYwiBkVEB_x"
   },
   "source": [
    "## Calculate the candidates from the covisitation matrices ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QiYVCoU1Z-I7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [03:08<00:00, 18.90s/it]\n",
      "100% 100/100 [00:28<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if covisitation:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/covisitation_parquet/wgt_*_top20*')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'}) \n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/covisitation', files=100, split_column = 'session')\n",
    "\n",
    "  del covisitation_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PJcysKJf3Q3F"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [02:12<00:00, 13.21s/it]\n",
      "100% 100/100 [00:22<00:00,  4.35it/s]\n"
     ]
    }
   ],
   "source": [
    "if cart_order:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/cart_order_parquet/*')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/cart_order', files=100, split_column = 'session')\n",
    "\n",
    "  del covisitation_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OFZUcPVq6v6Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 10/10 [00:13<00:00,  1.34s/it]\n",
      "100% 100/100 [00:02<00:00, 37.53it/s]\n"
     ]
    }
   ],
   "source": [
    "if also_buy:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/also_buy_parquet/*')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[\n",
    "        (reduced_df['session'] >= min(session_list))\n",
    "        & (reduced_df['session'] <= max(session_list))\n",
    "        & (reduced_df['source'].isin(['carts', 'orders']))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/also_buy', files=100, split_column = 'session')\n",
    "  del covisitation_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzQg51HBDny_"
   },
   "source": [
    "## Create the word2vec candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "uclzXsiR-xUN"
   },
   "outputs": [],
   "source": [
    "## Word2Vec functions:\n",
    "def get_session_vector(df, w2vec):\n",
    "  aids = df.aid.unique()\n",
    "  for i, aid in enumerate(aids):\n",
    "    vec = w2vec.wv[aid] if i == 0 else vec + w2vec.wv[aid]\n",
    "  vec = vec / len(aids)\n",
    "  return vec\n",
    "\n",
    "def get_close_aids(df, w2vec, index, idx2aid, n=20):\n",
    "  session_vec = get_session_vector(df, w2vec)\n",
    "  close_aids = get_nearest_neighbours(session_vec, index, idx2aid, n)\n",
    "  return close_aids\n",
    "\n",
    "def get_nearest_neighbours(x, index, idx2aid, n=20):\n",
    "  indexes, distances = index.get_nns_by_vector(x, n, search_k=-1, include_distances=True)\n",
    "  aids = [idx2aid[i] for i in indexes]\n",
    "  df = pd.DataFrame(data={'aid' : aids, 'w2vec_dist' : distances})\n",
    "  return df\n",
    "\n",
    "def get_word2vec_recs(train, test, n=20):\n",
    "  vector_size = 32\n",
    "  epochs = 9\n",
    "  sg = 1 # 1 for skip-gram\n",
    "  pop_thresh = 0.82415\n",
    "  window = 8\n",
    "  distance = 'angular'\n",
    "\n",
    "  reduced_df = pd.concat([train, test[['session','aid']]])\n",
    "  del train\n",
    "  sentences = reduced_df.groupby('session', as_index=False).agg({'aid' : lambda x: [str(i) for i in x.tolist()]}).rename(columns={'aid' : 'sentence'})\n",
    "  sentences = sentences['sentence'].to_list()\n",
    "\n",
    "  w2vec = Word2Vec(sentences=sentences, vector_size=vector_size, epochs = epochs, sg=sg, min_count=1, workers=14, window=window)\n",
    "\n",
    "  index = AnnoyIndex(vector_size, distance)\n",
    "  aid2idx = {}\n",
    "\n",
    "  popular_aids = test.groupby('aid', as_index=False).agg({'session' : 'count'})\n",
    "  popular_aids = popular_aids.loc[popular_aids['session'] > popular_aids['session'].quantile(pop_thresh)]\n",
    "  popular_aid_list = popular_aids.aid.unique()\n",
    "\n",
    "  for i, aid in enumerate(popular_aid_list):\n",
    "    aid = str(aid)\n",
    "    aid2idx[aid] = i\n",
    "    index.add_item(i, w2vec.wv[aid])\n",
    "  idx2aid = { v : k for k, v in aid2idx.items()}\n",
    "  index.build(40) # build 40 trees\n",
    "\n",
    "  reduced_test = test.copy()\n",
    "  reduced_test['aid'] = reduced_test['aid'].astype('str')\n",
    "  reduced_test['aid_vector'] = reduced_test['aid'].apply(lambda x: w2vec.wv[x])\n",
    "\n",
    "  reduced_test = reduced_test.groupby('session').apply(lambda x: get_close_aids(x, w2vec, index, idx2aid, n)).reset_index().drop(columns='level_1')\n",
    "  reduced_test['aid'] = reduced_test['aid'].astype('int32')\n",
    "  reduced_test['n'] = reduced_test.groupby('session').cumcount() + 1\n",
    "\n",
    "  return reduced_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163955180, 4)\n"
     ]
    }
   ],
   "source": [
    "if word2vec:\n",
    "    train = get_train(validation, sample_prop)\n",
    "    print(train.shape)\n",
    "    word2vec_recs = get_word2vec_recs(train, reduced_df, 100)\n",
    "    save_parquet(word2vec_recs, f'{path_to_candidate_features}/word2vec', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_recs = get_word2vec_recs(train, reduced_df, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_parquet(word2vec_recs, f'{path_to_candidate_features}/word2vec', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIBjso6IDq4T",
    "tags": []
   },
   "source": [
    "## Create the ALS candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y09HWhBm7Bc_"
   },
   "outputs": [],
   "source": [
    "## ALS functions\n",
    "def get_items_to_exclude(reduced_df, proportion=0):\n",
    "  ''' returns items with low popularity in the test set to exclude from predictions '''\n",
    "  items_to_exclude = reduced_df.loc[reduced_df['dataset'] == 'test'].groupby('item_id', as_index=False).agg({'test_set_actions' : 'sum'})\n",
    "  n = items_to_exclude['test_set_actions'].quantile(proportion)\n",
    "  items_to_exclude = items_to_exclude.loc[items_to_exclude['test_set_actions'] <= n]\n",
    "  items_to_exclude = items_to_exclude['item_id'].tolist()\n",
    "  return items_to_exclude\n",
    "\n",
    "def get_users_to_keep(reduced_df, n=0):\n",
    "  ''' get a list of all user codes with total interactions >= n '''\n",
    "  users_to_keep = reduced_df.groupby('user_id', as_index=False).agg({'aid' : 'count'})\n",
    "  users_to_keep = users_to_keep.loc[users_to_keep['aid'] >= n]\n",
    "  users_to_keep = users_to_keep.user_id.tolist()\n",
    "  return users_to_keep\n",
    "\n",
    "def get_als_recommendations(train, test, n_recs=20):\n",
    "  iterations = 2\n",
    "  factors = 800\n",
    "  regularization = 1.7050\n",
    "  minimum_clicks = 22\n",
    "  popularity_threshold = 0.10\n",
    "\n",
    "  train['dataset'] = 'train'\n",
    "  test['dataset'] = 'test'\n",
    "  reduced_df = pd.concat([train, test])\n",
    "  del train\n",
    "\n",
    "  reduced_df.reset_index(inplace=True)\n",
    "\n",
    "  reduced_df['user'] = reduced_df['session'].astype('category')\n",
    "  reduced_df['user_id'] = reduced_df['user'].cat.codes\n",
    "  reduced_df['item'] = reduced_df['aid'].astype('category')\n",
    "  reduced_df['item_id'] = reduced_df['item'].cat.codes\n",
    "  reduced_df['test_set_actions'] = 0\n",
    "  reduced_df.loc[reduced_df['dataset'] == 'test', 'test_set_actions'] = 1\n",
    "  reduced_df = convert_columns(reduced_df)\n",
    "\n",
    "  test_indices_start = len(reduced_df.loc[reduced_df['dataset'] == 'train'].session.unique())\n",
    "  test_indices_end = len(reduced_df.session.unique())\n",
    "  item_ids = {k: v for k, v in zip(reduced_df['item_id'], reduced_df['item'])}\n",
    "  validation_user_ids = [id for id in range(test_indices_start, test_indices_end)]\n",
    "  reduced_df.drop(columns=['user', 'ts', 'index','item'], inplace=True)\n",
    "\n",
    "  user_item = sps.coo_matrix(\n",
    "      (np.ones(reduced_df.shape[0]), # We're using a matrix of ones, but using type weights or repurchase weights could help!\n",
    "      (reduced_df['user_id'],\n",
    "      reduced_df['item_id'])),\n",
    "      dtype='int8'\n",
    "    ).tocsr()\n",
    "\n",
    "  model = implicit.als.AlternatingLeastSquares(\n",
    "      iterations = iterations,\n",
    "      factors=factors,\n",
    "      regularization=regularization,\n",
    "      dtype=np.float32\n",
    "  )\n",
    "\n",
    "  users_to_keep = get_users_to_keep(reduced_df, n=minimum_clicks)\n",
    "  items_to_exclude = get_items_to_exclude(reduced_df, proportion=popularity_threshold)\n",
    "\n",
    "  user_item_train = user_item[users_to_keep, :]\n",
    "\n",
    "  model.fit(user_item_train, show_progress=True)\n",
    "\n",
    "  args = {'userid' : validation_user_ids,\n",
    "          'user_items' : user_item[validation_user_ids,:],\n",
    "          'filter_items' : items_to_exclude,\n",
    "          'filter_already_liked_items' : False,\n",
    "          'recalculate_user' : True,\n",
    "          'N' : n_recs\n",
    "          }\n",
    "\n",
    "  recs = model.recommend(**args)\n",
    "\n",
    "  recs = pd.DataFrame(data={'session' : reduced_df.loc[reduced_df['dataset'] == 'test']['session'].unique(),\n",
    "                                'aid' : recs[0][:].tolist(),\n",
    "                                  'confidence' : recs[1][:].tolist()})\n",
    "  recs = recs.set_index('session').apply(pd.Series.explode).reset_index()\n",
    "  recs['aid'] = recs['aid'].map(item_ids)\n",
    "  recs['n'] = recs.groupby('session').cumcount() + 1\n",
    "  return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt8ZSrDWgp1Q"
   },
   "outputs": [],
   "source": [
    "if als:\n",
    "  train = get_train(validation, sample_prop, columns=['session', 'aid'])\n",
    "  als_recs = get_als_recommendations(train, reduced_df, 200)\n",
    "\n",
    "  save_parquet(als_recs, f'{path_to_candidate_features}/als', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxCx7eXDH4Qq+xMxO+vMlq",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
