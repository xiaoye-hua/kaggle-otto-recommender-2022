{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SdiYblJx2ao"
   },
   "source": [
    "# Generate Candidates #\n",
    "We take candidates from a range of sources:\n",
    "* Items already interacted with in the session\n",
    "* Our covisitation matrices\n",
    "* A word2vec model\n",
    "* An ALS recommender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rgw3DUBDx7hN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path to module: /home/jupyter/kaggle-otto-recommender-2022\n",
      "data path: /home/jupyter/kaggle-otto-recommender-2022/data\n",
      "/home/jupyter/kaggle-otto-recommender-2022/data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import data_path, path_to_module\n",
    "print(f\"path to module: {path_to_module}\")\n",
    "print(f\"data path: {data_path}\")\n",
    "import sys   \n",
    "sys.path.append(path_to_module)\n",
    "%cd {data_path}\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M9QNAWgIugYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: implicit in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from implicit) (4.64.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from implicit) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/conda/lib/python3.7/site-packages (from implicit) (1.7.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: Annoy in /opt/conda/lib/python3.7/site-packages (1.17.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting fastparquet\n",
      "  Downloading fastparquet-0.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /opt/conda/lib/python3.7/site-packages (from fastparquet) (1.21.6)\n",
      "Collecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from fastparquet) (1.3.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from fastparquet) (2023.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.0->fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->fastparquet) (1.16.0)\n",
      "Installing collected packages: cramjam, fastparquet\n",
      "Successfully installed cramjam-2.6.2 fastparquet-0.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install implicit\n",
    "!pip install Annoy\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bbpCdPcQOMqe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/implicit/gpu/__init__.py:14: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: CUDA driver version is insufficient for CUDA runtime version (/home/conda/feedstock_root/build_artifacts/implicit_1643471602441/work/./implicit/gpu/utils.h:71)'\n",
      "  f\"CUDA extension is built, but disabling GPU support because of '{e}'\",\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import seaborn as sns\n",
    "from otto_utils import get_train, get_test, convert_columns, save_parquet, make_directory, create_sub\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sps\n",
    "import implicit\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QZ6WwCocB3CD"
   },
   "outputs": [],
   "source": [
    "sample_prop = None\n",
    "validation = True\n",
    "covisitation = True\n",
    "cart_order = True\n",
    "also_buy = True\n",
    "als = True\n",
    "word2vec = True\n",
    "\n",
    "path_to_candidate_features = './train_candidate_features' if validation else './test_candidate_features'\n",
    "n=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f4EnH3CgZf89"
   },
   "outputs": [],
   "source": [
    "reduced_df = get_test(validation, sample_prop)\n",
    "reduced_df.rename(columns = {'type' : 'source'}, inplace=True)\n",
    "reduced_df['joiner'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7683577, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5soINLMjD-cK"
   },
   "source": [
    "## Calculate the candidates from items already in the basket ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./train_candidate_features'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_candidate_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tmAKeMmvTK5K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 100/100 [00:01<00:00, 66.26it/s]\n"
     ]
    }
   ],
   "source": [
    "training_skeleton = reduced_df\n",
    "training_skeleton['time_weight'] = training_skeleton.groupby('session')['aid'].transform(lambda x: np.logspace(0.1, 1, x.shape[0], base=2, endpoint=True)) - 1 # Changing this from 0.1 to 0.5 impacts recall by 0.01\n",
    "training_skeleton['type_weight'] = 1\n",
    "training_skeleton.loc[training_skeleton.source == 'carts', 'type_weight'] = 3\n",
    "training_skeleton.loc[training_skeleton.source == 'orders', 'type_weight'] = 6\n",
    "training_skeleton['weight'] = training_skeleton['time_weight'] * training_skeleton['type_weight']\n",
    "training_skeleton = training_skeleton.groupby(['session', 'aid'], as_index=False).agg({'weight' : 'sum'})\n",
    "training_skeleton.sort_values(by=['session', 'weight'], ascending=[True, False], inplace=True)\n",
    "training_skeleton['n_basket'] = training_skeleton.groupby('session').cumcount() + 1\n",
    "training_skeleton = convert_columns(training_skeleton)\n",
    "\n",
    "save_parquet(training_skeleton, f'{path_to_candidate_features}/basket', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMYwiBkVEB_x"
   },
   "source": [
    "## Calculate the candidates from the covisitation matrices ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{path_to_candidate_features}/also_buy_parquet/*.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train_candidate_features/also_buy_parquet/top_15_buy2buy_v5_0.pqt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/kaggle-otto-recommender-2022/data'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_15_carts_orders_v5_0.pqt  top_15_carts_orders_v5_2.pqt\n",
      "top_15_carts_orders_v5_1.pqt  top_15_carts_orders_v5_3.pqt\n"
     ]
    }
   ],
   "source": [
    "%ls train_candidate_features/cart_order_parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.21.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/kaggle-otto-recommender-2022/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls also_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QiYVCoU1Z-I7"
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Error creating dataset. Could not read schema from './train_candidate_features/covisitation_parquet/top_20_clicks_v5_0.pqt': Could not open Parquet input source './train_candidate_features/covisitation_parquet/top_20_clicks_v5_0.pqt': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.. Is this a 'parquet' file?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125/3909337373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcovisitation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_to_candidate_features}/covisitation_parquet/top_20_clicks_*_*.pqt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mcovisitation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'aid_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aid_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcovisitation_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovisitation_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             result = self.api.parquet.read_table(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mignore_prefixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_prefixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0mpre_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, **kwargs)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                                    \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                                    \u001b[0mpartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1729\u001b[0;31m                                    ignore_prefixes=ignore_prefixes)\n\u001b[0m\u001b[1;32m   1730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\u001b[0m in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_is_path_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_filesystem_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_union_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\u001b[0m in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileSystemDatasetFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_or_selector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.DatasetFactory.finish\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Error creating dataset. Could not read schema from './train_candidate_features/covisitation_parquet/top_20_clicks_v5_0.pqt': Could not open Parquet input source './train_candidate_features/covisitation_parquet/top_20_clicks_v5_0.pqt': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.. Is this a 'parquet' file?"
     ]
    }
   ],
   "source": [
    "if covisitation:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/covisitation_parquet/top_20_clicks_*_*.pqt')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'}) \n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/covisitation', files=100, split_column = 'session')\n",
    "\n",
    "  del covisitation_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJcysKJf3Q3F"
   },
   "outputs": [],
   "source": [
    "if cart_order:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/cart_order_parquet/*')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[(reduced_df['session'] >= min(session_list)) & (reduced_df['session'] <= max(session_list))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/cart_order', files=100, split_column = 'session')\n",
    "\n",
    "  del covisitation_options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "also_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train_candidate_features/also_buy_parquet/top_15_buy2buy_v5_0.pqt']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot construct a FileSource from a path without a FileSystem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: cannot construct a FileSource from a path without a FileSystem"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pyarrow._dataset._make_file_source'\n",
      "ValueError: cannot construct a FileSource from a path without a FileSystem\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Called Open() on an uninitialized FileSource",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125/2859377066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             result = self.api.parquet.read_table(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit)\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mignore_prefixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_prefixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m                 \u001b[0mpre_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_int96_timestamp_unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             )\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, **kwargs)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m             self._dataset = ds.FileSystemDataset(\n\u001b[0;32m-> 1711\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1712\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/_dataset.pyx\u001b[0m in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Called Open() on an uninitialized FileSource"
     ]
    }
   ],
   "source": [
    "pd.read_parquet(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OFZUcPVq6v6Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['pairings'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125/4284408440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aid_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aid'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pairings'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pairings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'aid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ts'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupByApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_dict_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# we require a list, but not a 'str'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mselection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_dictlike_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"agg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselected_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mnormalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mcols_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column(s) {cols_sorted} do not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mis_aggregator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['pairings'] do not exist\""
     ]
    }
   ],
   "source": [
    "if also_buy:\n",
    "  files = glob.glob(f'{path_to_candidate_features}/also_buy_parquet/*')\n",
    "  covisitation_matrix = convert_columns(pd.read_parquet(files))\n",
    "  for column in ['aid_x', 'aid_y']:\n",
    "    covisitation_matrix[column] = covisitation_matrix[column].astype('int32')\n",
    "\n",
    "  sessions = reduced_df['session'].unique()\n",
    "  sessions.sort()\n",
    "  session_lists = [np_array.tolist() for np_array in np.array_split(np.array(sessions), 10 ) ]\n",
    "\n",
    "  covisitation_list = []\n",
    "  for i, session_list in enumerate(tqdm(session_lists)):\n",
    "    chunk = reduced_df.loc[\n",
    "        (reduced_df['session'] >= min(session_list))\n",
    "        & (reduced_df['session'] <= max(session_list))\n",
    "        & (reduced_df['source'].isin(['carts', 'orders']))]\n",
    "    covisitation_options = (\n",
    "        chunk.merge(\n",
    "            covisitation_matrix,\n",
    "            how='left',\n",
    "            left_on = ['aid'],\n",
    "            right_on = ['aid_x']).drop_duplicates(subset=['session', 'aid_x', 'aid_y'], keep='last')\n",
    "        .groupby(['session', 'aid_y'], as_index=False)\n",
    "        .agg({'ts': 'max', 'aid' : 'count', 'pairings' : 'sum'})\n",
    "        .sort_values(by=['session', 'aid', 'ts', 'pairings'], ascending=[True, False, False, False])\n",
    "        .drop(columns={'aid', 'ts'})\n",
    "        .rename(columns={'aid_y' : 'aid'})\n",
    "    )\n",
    "    covisitation_options['n'] = covisitation_options.groupby('session').cumcount() + 1\n",
    "    covisitation_options = covisitation_options.loc[covisitation_options['n'] <= 150]\n",
    "\n",
    "    #covisitation_options.drop(columns='n', inplace=True)\n",
    "    for column in ['aid', 'session']:\n",
    "      covisitation_options[column] = covisitation_options[column].astype('int32')\n",
    "    covisitation_list.append(covisitation_options)\n",
    "    del chunk\n",
    "\n",
    "  covisitation_options = pd.concat(covisitation_list)\n",
    "  del covisitation_list\n",
    "\n",
    "  save_parquet(covisitation_options, f'{path_to_candidate_features}/also_buy', files=100, split_column = 'session')\n",
    "  del covisitation_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzQg51HBDny_"
   },
   "source": [
    "## Create the word2vec candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uclzXsiR-xUN"
   },
   "outputs": [],
   "source": [
    "## Word2Vec functions:\n",
    "def get_session_vector(df, w2vec):\n",
    "  aids = df.aid.unique()\n",
    "  for i, aid in enumerate(aids):\n",
    "    vec = w2vec.wv[aid] if i == 0 else vec + w2vec.wv[aid]\n",
    "  vec = vec / len(aids)\n",
    "  return vec\n",
    "\n",
    "def get_close_aids(df, w2vec, index, idx2aid, n=20):\n",
    "  session_vec = get_session_vector(df, w2vec)\n",
    "  close_aids = get_nearest_neighbours(session_vec, index, idx2aid, n)\n",
    "  return close_aids\n",
    "\n",
    "def get_nearest_neighbours(x, index, idx2aid, n=20):\n",
    "  indexes, distances = index.get_nns_by_vector(x, n, search_k=-1, include_distances=True)\n",
    "  aids = [idx2aid[i] for i in indexes]\n",
    "  df = pd.DataFrame(data={'aid' : aids, 'w2vec_dist' : distances})\n",
    "  return df\n",
    "\n",
    "def get_word2vec_recs(train, test, n=20):\n",
    "  vector_size = 32\n",
    "  epochs = 9\n",
    "  sg = 1\n",
    "  pop_thresh = 0.82415\n",
    "  window = 8\n",
    "  distance = 'angular'\n",
    "\n",
    "  reduced_df = pd.concat([train, test[['session','aid']]])\n",
    "  del train\n",
    "  sentences = reduced_df.groupby('session', as_index=False).agg({'aid' : lambda x: [str(i) for i in x.tolist()]}).rename(columns={'aid' : 'sentence'})\n",
    "  sentences = sentences['sentence'].to_list()\n",
    "\n",
    "  w2vec = Word2Vec(sentences=sentences, size=vector_size, iter = epochs, sg=sg, min_count=1, workers=14, window=window)\n",
    "\n",
    "  index = AnnoyIndex(vector_size, distance)\n",
    "  aid2idx = {}\n",
    "\n",
    "  popular_aids = test.groupby('aid', as_index=False).agg({'session' : 'count'})\n",
    "  popular_aids = popular_aids.loc[popular_aids['session'] > popular_aids['session'].quantile(pop_thresh)]\n",
    "  popular_aid_list = popular_aids.aid.unique()\n",
    "\n",
    "  for i, aid in enumerate(popular_aid_list):\n",
    "    aid = str(aid)\n",
    "    aid2idx[aid] = i\n",
    "    index.add_item(i, w2vec.wv[aid])\n",
    "  idx2aid = { v : k for k, v in aid2idx.items()}\n",
    "  index.build(40)\n",
    "\n",
    "  reduced_test = test.copy()\n",
    "  reduced_test['aid'] = reduced_test['aid'].astype('str')\n",
    "  reduced_test['aid_vector'] = reduced_test['aid'].apply(lambda x: w2vec.wv[x])\n",
    "\n",
    "  reduced_test = reduced_test.groupby('session').apply(lambda x: get_close_aids(x, w2vec, index, idx2aid, n)).reset_index().drop(columns='level_1')\n",
    "  reduced_test['aid'] = reduced_test['aid'].astype('int32')\n",
    "  reduced_test['n'] = reduced_test.groupby('session').cumcount() + 1\n",
    "\n",
    "  return reduced_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qILPphPwN7au"
   },
   "outputs": [],
   "source": [
    "if word2vec:\n",
    "  train = get_train(validation, sample_prop)\n",
    "  word2vec_recs = get_word2vec_recs(train, reduced_df, 100)\n",
    "  \n",
    "  save_parquet(word2vec_recs, f'{path_to_candidate_features}/word2vec', files=100, split_column = 'session')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIBjso6IDq4T"
   },
   "source": [
    "## Create the ALS candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y09HWhBm7Bc_"
   },
   "outputs": [],
   "source": [
    "## ALS functions\n",
    "def get_items_to_exclude(reduced_df, proportion=0):\n",
    "  ''' returns items with low popularity in the test set to exclude from predictions '''\n",
    "  items_to_exclude = reduced_df.loc[reduced_df['dataset'] == 'test'].groupby('item_id', as_index=False).agg({'test_set_actions' : 'sum'})\n",
    "  n = items_to_exclude['test_set_actions'].quantile(proportion)\n",
    "  items_to_exclude = items_to_exclude.loc[items_to_exclude['test_set_actions'] <= n]\n",
    "  items_to_exclude = items_to_exclude['item_id'].tolist()\n",
    "  return items_to_exclude\n",
    "\n",
    "def get_users_to_keep(reduced_df, n=0):\n",
    "  ''' get a list of all user codes with total interactions >= n '''\n",
    "  users_to_keep = reduced_df.groupby('user_id', as_index=False).agg({'aid' : 'count'})\n",
    "  users_to_keep = users_to_keep.loc[users_to_keep['aid'] >= n]\n",
    "  users_to_keep = users_to_keep.user_id.tolist()\n",
    "  return users_to_keep\n",
    "\n",
    "def get_als_recommendations(train, test, n_recs=20):\n",
    "  iterations = 2\n",
    "  factors = 800\n",
    "  regularization = 1.7050\n",
    "  minimum_clicks = 22\n",
    "  popularity_threshold = 0.10\n",
    "\n",
    "  train['dataset'] = 'train'\n",
    "  test['dataset'] = 'test'\n",
    "  reduced_df = pd.concat([train, test])\n",
    "  del train\n",
    "\n",
    "  reduced_df.reset_index(inplace=True)\n",
    "\n",
    "  reduced_df['user'] = reduced_df['session'].astype('category')\n",
    "  reduced_df['user_id'] = reduced_df['user'].cat.codes\n",
    "  reduced_df['item'] = reduced_df['aid'].astype('category')\n",
    "  reduced_df['item_id'] = reduced_df['item'].cat.codes\n",
    "  reduced_df['test_set_actions'] = 0\n",
    "  reduced_df.loc[reduced_df['dataset'] == 'test', 'test_set_actions'] = 1\n",
    "  reduced_df = convert_columns(reduced_df)\n",
    "\n",
    "  test_indices_start = len(reduced_df.loc[reduced_df['dataset'] == 'train'].session.unique())\n",
    "  test_indices_end = len(reduced_df.session.unique())\n",
    "  item_ids = {k: v for k, v in zip(reduced_df['item_id'], reduced_df['item'])}\n",
    "  validation_user_ids = [id for id in range(test_indices_start, test_indices_end)]\n",
    "  reduced_df.drop(columns=['user', 'ts', 'index','item'], inplace=True)\n",
    "\n",
    "  user_item = sps.coo_matrix(\n",
    "      (np.ones(reduced_df.shape[0]), # We're using a matrix of ones, but using type weights or repurchase weights could help!\n",
    "      (reduced_df['user_id'],\n",
    "      reduced_df['item_id'])),\n",
    "      dtype='int8'\n",
    "    ).tocsr()\n",
    "\n",
    "  model = implicit.als.AlternatingLeastSquares(\n",
    "      iterations = iterations,\n",
    "      factors=factors,\n",
    "      regularization=regularization,\n",
    "      dtype=np.float32\n",
    "  )\n",
    "\n",
    "  users_to_keep = get_users_to_keep(reduced_df, n=minimum_clicks)\n",
    "  items_to_exclude = get_items_to_exclude(reduced_df, proportion=popularity_threshold)\n",
    "\n",
    "  user_item_train = user_item[users_to_keep, :]\n",
    "\n",
    "  model.fit(user_item_train, show_progress=True)\n",
    "\n",
    "  args = {'userid' : validation_user_ids,\n",
    "          'user_items' : user_item[validation_user_ids,:],\n",
    "          'filter_items' : items_to_exclude,\n",
    "          'filter_already_liked_items' : False,\n",
    "          'recalculate_user' : True,\n",
    "          'N' : n_recs\n",
    "          }\n",
    "\n",
    "  recs = model.recommend(**args)\n",
    "\n",
    "  recs = pd.DataFrame(data={'session' : reduced_df.loc[reduced_df['dataset'] == 'test']['session'].unique(),\n",
    "                                'aid' : recs[0][:].tolist(),\n",
    "                                  'confidence' : recs[1][:].tolist()})\n",
    "  recs = recs.set_index('session').apply(pd.Series.explode).reset_index()\n",
    "  recs['aid'] = recs['aid'].map(item_ids)\n",
    "  recs['n'] = recs.groupby('session').cumcount() + 1\n",
    "  return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt8ZSrDWgp1Q"
   },
   "outputs": [],
   "source": [
    "if als:\n",
    "  train = get_train(validation, sample_prop, columns=['session', 'aid'])\n",
    "  als_recs = get_als_recommendations(train, reduced_df, 200)\n",
    "\n",
    "  save_parquet(als_recs, f'{path_to_candidate_features}/als', files=100, split_column = 'session')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxCx7eXDH4Qq+xMxO+vMlq",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
